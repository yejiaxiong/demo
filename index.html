<!doctype html>
<html>
<head>
<meta charset='UTF-8'><meta name='viewport' content='width=device-width initial-scale=1'>

<link href='https://fonts.googleapis.com/css?family=Open+Sans:400italic,700italic,700,400&subset=latin,latin-ext' rel='stylesheet' type='text/css' /><style type='text/css'>html {overflow-x: initial !important;}:root { --bg-color:#ffffff; --text-color:#333333; --select-text-bg-color:#B5D6FC; --select-text-font-color:auto; --monospace:"Lucida Console",Consolas,"Courier",monospace; --title-bar-height:20px; }
.mac-os-11 { --title-bar-height:28px; }
html { font-size: 14px; background-color: var(--bg-color); color: var(--text-color); font-family: "Helvetica Neue", Helvetica, Arial, sans-serif; -webkit-font-smoothing: antialiased; }
h1, h2, h3, h4, h5 { white-space: pre-wrap; }
body { margin: 0px; padding: 0px; height: auto; inset: 0px; font-size: 1rem; line-height: 1.42857; overflow-x: hidden; background: inherit; tab-size: 4; }
iframe { margin: auto; }
a.url { word-break: break-all; }
a:active, a:hover { outline: 0px; }
.in-text-selection, ::selection { text-shadow: none; background: var(--select-text-bg-color); color: var(--select-text-font-color); }
#write { margin: 0px auto; height: auto; width: inherit; word-break: normal; overflow-wrap: break-word; position: relative; white-space: normal; overflow-x: visible; padding-top: 36px; }
#write.first-line-indent p { text-indent: 2em; }
#write.first-line-indent li p, #write.first-line-indent p * { text-indent: 0px; }
#write.first-line-indent li { margin-left: 2em; }
.for-image #write { padding-left: 8px; padding-right: 8px; }
body.typora-export { padding-left: 30px; padding-right: 30px; }
.typora-export .footnote-line, .typora-export li, .typora-export p { white-space: pre-wrap; }
.typora-export .task-list-item input { pointer-events: none; }
@media screen and (max-width: 500px) {
  body.typora-export { padding-left: 0px; padding-right: 0px; }
  #write { padding-left: 20px; padding-right: 20px; }
}
#write li > figure:last-child { margin-bottom: 0.5rem; }
#write ol, #write ul { position: relative; }
img { max-width: 100%; vertical-align: middle; image-orientation: from-image; }
button, input, select, textarea { color: inherit; font: inherit; }
input[type="checkbox"], input[type="radio"] { line-height: normal; padding: 0px; }
*, ::after, ::before { box-sizing: border-box; }
#write h1, #write h2, #write h3, #write h4, #write h5, #write h6, #write p, #write pre { width: inherit; }
#write h1, #write h2, #write h3, #write h4, #write h5, #write h6, #write p { position: relative; }
p { line-height: inherit; }
h1, h2, h3, h4, h5, h6 { break-after: avoid-page; break-inside: avoid; orphans: 4; }
p { orphans: 4; }
h1 { font-size: 2rem; }
h2 { font-size: 1.8rem; }
h3 { font-size: 1.6rem; }
h4 { font-size: 1.4rem; }
h5 { font-size: 1.2rem; }
h6 { font-size: 1rem; }
.md-math-block, .md-rawblock, h1, h2, h3, h4, h5, h6, p { margin-top: 1rem; margin-bottom: 1rem; }
.hidden { display: none; }
.md-blockmeta { color: rgb(204, 204, 204); font-weight: 700; font-style: italic; }
a { cursor: pointer; }
sup.md-footnote { padding: 2px 4px; background-color: rgba(238, 238, 238, 0.7); color: rgb(85, 85, 85); border-radius: 4px; cursor: pointer; }
sup.md-footnote a, sup.md-footnote a:hover { color: inherit; text-transform: inherit; text-decoration: inherit; }
#write input[type="checkbox"] { cursor: pointer; width: inherit; height: inherit; }
figure { overflow-x: auto; margin: 1.2em 0px; max-width: calc(100% + 16px); padding: 0px; }
figure > table { margin: 0px; }
thead, tr { break-inside: avoid; break-after: auto; }
thead { display: table-header-group; }
table { border-collapse: collapse; border-spacing: 0px; width: 100%; overflow: auto; break-inside: auto; text-align: left; }
table.md-table td { min-width: 32px; }
.CodeMirror-gutters { border-right: 0px; background-color: inherit; }
.CodeMirror-linenumber { user-select: none; }
.CodeMirror { text-align: left; }
.CodeMirror-placeholder { opacity: 0.3; }
.CodeMirror pre { padding: 0px 4px; }
.CodeMirror-lines { padding: 0px; }
div.hr:focus { cursor: none; }
#write pre { white-space: pre-wrap; }
#write.fences-no-line-wrapping pre { white-space: pre; }
#write pre.ty-contain-cm { white-space: normal; }
.CodeMirror-gutters { margin-right: 4px; }
.md-fences { font-size: 0.9rem; display: block; break-inside: avoid; text-align: left; overflow: visible; white-space: pre; background: inherit; position: relative !important; }
.md-fences-adv-panel { width: 100%; margin-top: 10px; text-align: center; padding-top: 0px; padding-bottom: 8px; overflow-x: auto; }
#write .md-fences.mock-cm { white-space: pre-wrap; }
.md-fences.md-fences-with-lineno { padding-left: 0px; }
#write.fences-no-line-wrapping .md-fences.mock-cm { white-space: pre; overflow-x: auto; }
.md-fences.mock-cm.md-fences-with-lineno { padding-left: 8px; }
.CodeMirror-line, twitterwidget { break-inside: avoid; }
svg { break-inside: avoid; }
.footnotes { opacity: 0.8; font-size: 0.9rem; margin-top: 1em; margin-bottom: 1em; }
.footnotes + .footnotes { margin-top: 0px; }
.md-reset { margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: top; background: 0px 0px; text-decoration: none; text-shadow: none; float: none; position: static; width: auto; height: auto; white-space: nowrap; cursor: inherit; -webkit-tap-highlight-color: transparent; line-height: normal; font-weight: 400; text-align: left; box-sizing: content-box; direction: ltr; }
li div { padding-top: 0px; }
blockquote { margin: 1rem 0px; }
li .mathjax-block, li p { margin: 0.5rem 0px; }
li blockquote { margin: 1rem 0px; }
li { margin: 0px; position: relative; }
blockquote > :last-child { margin-bottom: 0px; }
blockquote > :first-child, li > :first-child { margin-top: 0px; }
.footnotes-area { color: rgb(136, 136, 136); margin-top: 0.714rem; padding-bottom: 0.143rem; white-space: normal; }
#write .footnote-line { white-space: pre-wrap; }
@media print {
  body, html { border: 1px solid transparent; height: 99%; break-after: avoid; break-before: avoid; font-variant-ligatures: no-common-ligatures; }
  #write { margin-top: 0px; border-color: transparent !important; padding-top: 0px !important; padding-bottom: 0px !important; }
  .typora-export * { -webkit-print-color-adjust: exact; }
  .typora-export #write { break-after: avoid; }
  .typora-export #write::after { height: 0px; }
  .is-mac table { break-inside: avoid; }
  #write > p:nth-child(1) { margin-top: 0px; }
  .typora-export-show-outline .typora-export-sidebar { display: none; }
  figure { overflow-x: visible; }
}
.footnote-line { margin-top: 0.714em; font-size: 0.7em; }
a img, img a { cursor: pointer; }
pre.md-meta-block { font-size: 0.8rem; min-height: 0.8rem; white-space: pre-wrap; background: rgb(204, 204, 204); display: block; overflow-x: hidden; }
p > .md-image:only-child:not(.md-img-error) img, p > img:only-child { display: block; margin: auto; }
#write.first-line-indent p > .md-image:only-child:not(.md-img-error) img { left: -2em; position: relative; }
p > .md-image:only-child { display: inline-block; width: 100%; }
#write .MathJax_Display { margin: 0.8em 0px 0px; }
.md-math-block { width: 100%; }
.md-math-block:not(:empty)::after { display: none; }
.MathJax_ref { fill: currentcolor; }
[contenteditable="true"]:active, [contenteditable="true"]:focus, [contenteditable="false"]:active, [contenteditable="false"]:focus { outline: 0px; box-shadow: none; }
.md-task-list-item { position: relative; list-style-type: none; }
.task-list-item.md-task-list-item { padding-left: 0px; }
.md-task-list-item > input { position: absolute; top: 0px; left: 0px; margin-left: -1.2em; margin-top: calc(1em - 10px); border: none; }
.math { font-size: 1rem; }
.md-toc { min-height: 3.58rem; position: relative; font-size: 0.9rem; border-radius: 10px; }
.md-toc-content { position: relative; margin-left: 0px; }
.md-toc-content::after, .md-toc::after { display: none; }
.md-toc-item { display: block; color: rgb(65, 131, 196); }
.md-toc-item a { text-decoration: none; }
.md-toc-inner:hover { text-decoration: underline; }
.md-toc-inner { display: inline-block; cursor: pointer; }
.md-toc-h1 .md-toc-inner { margin-left: 0px; font-weight: 700; }
.md-toc-h2 .md-toc-inner { margin-left: 2em; }
.md-toc-h3 .md-toc-inner { margin-left: 4em; }
.md-toc-h4 .md-toc-inner { margin-left: 6em; }
.md-toc-h5 .md-toc-inner { margin-left: 8em; }
.md-toc-h6 .md-toc-inner { margin-left: 10em; }
@media screen and (max-width: 48em) {
  .md-toc-h3 .md-toc-inner { margin-left: 3.5em; }
  .md-toc-h4 .md-toc-inner { margin-left: 5em; }
  .md-toc-h5 .md-toc-inner { margin-left: 6.5em; }
  .md-toc-h6 .md-toc-inner { margin-left: 8em; }
}
a.md-toc-inner { font-size: inherit; font-style: inherit; font-weight: inherit; line-height: inherit; }
.footnote-line a:not(.reversefootnote) { color: inherit; }
.reversefootnote { font-family: ui-monospace, sans-serif; }
.md-attr { display: none; }
.md-fn-count::after { content: "."; }
code, pre, samp, tt { font-family: var(--monospace); }
kbd { margin: 0px 0.1em; padding: 0.1em 0.6em; font-size: 0.8em; color: rgb(36, 39, 41); background: rgb(255, 255, 255); border: 1px solid rgb(173, 179, 185); border-radius: 3px; box-shadow: rgba(12, 13, 14, 0.2) 0px 1px 0px, rgb(255, 255, 255) 0px 0px 0px 2px inset; white-space: nowrap; vertical-align: middle; }
.md-comment { color: rgb(162, 127, 3); opacity: 0.6; font-family: var(--monospace); }
code { text-align: left; vertical-align: initial; }
a.md-print-anchor { white-space: pre !important; border-width: initial !important; border-style: none !important; border-color: initial !important; display: inline-block !important; position: absolute !important; width: 1px !important; right: 0px !important; outline: 0px !important; background: 0px 0px !important; text-decoration: initial !important; text-shadow: initial !important; }
.os-windows.monocolor-emoji .md-emoji { font-family: "Segoe UI Symbol", sans-serif; }
.md-diagram-panel > svg { max-width: 100%; }
[lang="flow"] svg, [lang="mermaid"] svg { max-width: 100%; height: auto; }
[lang="mermaid"] .node text { font-size: 1rem; }
table tr th { border-bottom: 0px; }
video { max-width: 100%; display: block; margin: 0px auto; }
iframe { max-width: 100%; width: 100%; border: none; }
.highlight td, .highlight tr { border: 0px; }
mark { background: rgb(255, 255, 0); color: rgb(0, 0, 0); }
.md-html-inline .md-plain, .md-html-inline strong, mark .md-inline-math, mark strong { color: inherit; }
.md-expand mark .md-meta { opacity: 0.3 !important; }
mark .md-meta { color: rgb(0, 0, 0); }
@media print {
  .typora-export h1, .typora-export h2, .typora-export h3, .typora-export h4, .typora-export h5, .typora-export h6 { break-inside: avoid; }
}
.md-diagram-panel .messageText { stroke: none !important; }
.md-diagram-panel .start-state { fill: var(--node-fill); }
.md-diagram-panel .edgeLabel rect { opacity: 1 !important; }
.md-fences.md-fences-math { font-size: 1em; }
.md-fences-advanced:not(.md-focus) { padding: 0px; white-space: nowrap; border: 0px; }
.md-fences-advanced:not(.md-focus) { background: inherit; }
.typora-export-show-outline .typora-export-content { max-width: 1440px; margin: auto; display: flex; flex-direction: row; }
.typora-export-sidebar { width: 300px; font-size: 0.8rem; margin-top: 80px; margin-right: 18px; }
.typora-export-show-outline #write { --webkit-flex:2; flex: 2 1 0%; }
.typora-export-sidebar .outline-content { position: fixed; top: 0px; max-height: 100%; overflow: hidden auto; padding-bottom: 30px; padding-top: 60px; width: 300px; }
@media screen and (max-width: 1024px) {
  .typora-export-sidebar, .typora-export-sidebar .outline-content { width: 240px; }
}
@media screen and (max-width: 800px) {
  .typora-export-sidebar { display: none; }
}
.outline-content li, .outline-content ul { margin-left: 0px; margin-right: 0px; padding-left: 0px; padding-right: 0px; list-style: none; overflow-wrap: anywhere; }
.outline-content ul { margin-top: 0px; margin-bottom: 0px; }
.outline-content strong { font-weight: 400; }
.outline-expander { width: 1rem; height: 1.42857rem; position: relative; display: table-cell; vertical-align: middle; cursor: pointer; padding-left: 4px; }
.outline-expander::before { content: ""; position: relative; font-family: Ionicons; display: inline-block; font-size: 8px; vertical-align: middle; }
.outline-item { padding-top: 3px; padding-bottom: 3px; cursor: pointer; }
.outline-expander:hover::before { content: ""; }
.outline-h1 > .outline-item { padding-left: 0px; }
.outline-h2 > .outline-item { padding-left: 1em; }
.outline-h3 > .outline-item { padding-left: 2em; }
.outline-h4 > .outline-item { padding-left: 3em; }
.outline-h5 > .outline-item { padding-left: 4em; }
.outline-h6 > .outline-item { padding-left: 5em; }
.outline-label { cursor: pointer; display: table-cell; vertical-align: middle; text-decoration: none; color: inherit; }
.outline-label:hover { text-decoration: underline; }
.outline-item:hover { border-color: rgb(245, 245, 245); background-color: var(--item-hover-bg-color); }
.outline-item:hover { margin-left: -28px; margin-right: -28px; border-left: 28px solid transparent; border-right: 28px solid transparent; }
.outline-item-single .outline-expander::before, .outline-item-single .outline-expander:hover::before { display: none; }
.outline-item-open > .outline-item > .outline-expander::before { content: ""; }
.outline-children { display: none; }
.info-panel-tab-wrapper { display: none; }
.outline-item-open > .outline-children { display: block; }
.typora-export .outline-item { padding-top: 1px; padding-bottom: 1px; }
.typora-export .outline-item:hover { margin-right: -8px; border-right: 8px solid transparent; }
.typora-export .outline-expander::before { content: "+"; font-family: inherit; top: -1px; }
.typora-export .outline-expander:hover::before, .typora-export .outline-item-open > .outline-item > .outline-expander::before { content: "−"; }
.typora-export-collapse-outline .outline-children { display: none; }
.typora-export-collapse-outline .outline-item-open > .outline-children, .typora-export-no-collapse-outline .outline-children { display: block; }
.typora-export-no-collapse-outline .outline-expander::before { content: "" !important; }
.typora-export-show-outline .outline-item-active > .outline-item .outline-label { font-weight: 700; }
.md-inline-math-container mjx-container { zoom: 0.95; }
mjx-container { break-inside: avoid; }


:root {
    --side-bar-bg-color: #fafafa;
    --control-text-color: #777;
}

@include-when-export url(https://fonts.googleapis.com/css?family=Open+Sans:400italic,700italic,700,400&subset=latin,latin-ext);

/* open-sans-regular - latin-ext_latin */
  /* open-sans-italic - latin-ext_latin */
    /* open-sans-700 - latin-ext_latin */
    /* open-sans-700italic - latin-ext_latin */
  html {
    font-size: 16px;
    -webkit-font-smoothing: antialiased;
}

body {
    font-family: "Open Sans","Clear Sans", "Helvetica Neue", Helvetica, Arial, 'Segoe UI Emoji', sans-serif;
    color: rgb(51, 51, 51);
    line-height: 1.6;
}

#write {
    max-width: 860px;
  	margin: 0 auto;
  	padding: 30px;
    padding-bottom: 100px;
}

@media only screen and (min-width: 1400px) {
	#write {
		max-width: 1024px;
	}
}

@media only screen and (min-width: 1800px) {
	#write {
		max-width: 1200px;
	}
}

#write > ul:first-child,
#write > ol:first-child{
    margin-top: 30px;
}

a {
    color: #4183C4;
}
h1,
h2,
h3,
h4,
h5,
h6 {
    position: relative;
    margin-top: 1rem;
    margin-bottom: 1rem;
    font-weight: bold;
    line-height: 1.4;
    cursor: text;
}
h1:hover a.anchor,
h2:hover a.anchor,
h3:hover a.anchor,
h4:hover a.anchor,
h5:hover a.anchor,
h6:hover a.anchor {
    text-decoration: none;
}
h1 tt,
h1 code {
    font-size: inherit;
}
h2 tt,
h2 code {
    font-size: inherit;
}
h3 tt,
h3 code {
    font-size: inherit;
}
h4 tt,
h4 code {
    font-size: inherit;
}
h5 tt,
h5 code {
    font-size: inherit;
}
h6 tt,
h6 code {
    font-size: inherit;
}
h1 {
    font-size: 2.25em;
    line-height: 1.2;
    border-bottom: 1px solid #eee;
}
h2 {
    font-size: 1.75em;
    line-height: 1.225;
    border-bottom: 1px solid #eee;
}

/*@media print {
    .typora-export h1,
    .typora-export h2 {
        border-bottom: none;
        padding-bottom: initial;
    }

    .typora-export h1::after,
    .typora-export h2::after {
        content: "";
        display: block;
        height: 100px;
        margin-top: -96px;
        border-top: 1px solid #eee;
    }
}*/

h3 {
    font-size: 1.5em;
    line-height: 1.43;
}
h4 {
    font-size: 1.25em;
}
h5 {
    font-size: 1em;
}
h6 {
   font-size: 1em;
    color: #777;
}
p,
blockquote,
ul,
ol,
dl,
table{
    margin: 0.8em 0;
}
li>ol,
li>ul {
    margin: 0 0;
}
hr {
    height: 2px;
    padding: 0;
    margin: 16px 0;
    background-color: #e7e7e7;
    border: 0 none;
    overflow: hidden;
    box-sizing: content-box;
}

li p.first {
    display: inline-block;
}
ul,
ol {
    padding-left: 30px;
}
ul:first-child,
ol:first-child {
    margin-top: 0;
}
ul:last-child,
ol:last-child {
    margin-bottom: 0;
}
blockquote {
    border-left: 4px solid #dfe2e5;
    padding: 0 15px;
    color: #777777;
}
blockquote blockquote {
    padding-right: 0;
}
table {
    padding: 0;
    word-break: initial;
}
table tr {
    border: 1px solid #dfe2e5;
    margin: 0;
    padding: 0;
}
table tr:nth-child(2n),
thead {
    background-color: #f8f8f8;
}
table th {
    font-weight: bold;
    border: 1px solid #dfe2e5;
    border-bottom: 0;
    margin: 0;
    padding: 6px 13px;
}
table td {
    border: 1px solid #dfe2e5;
    margin: 0;
    padding: 6px 13px;
}
table th:first-child,
table td:first-child {
    margin-top: 0;
}
table th:last-child,
table td:last-child {
    margin-bottom: 0;
}

.CodeMirror-lines {
    padding-left: 4px;
}

.code-tooltip {
    box-shadow: 0 1px 1px 0 rgba(0,28,36,.3);
    border-top: 1px solid #eef2f2;
}

.md-fences,
code,
tt {
    border: 1px solid #e7eaed;
    background-color: #f8f8f8;
    border-radius: 3px;
    padding: 0;
    padding: 2px 4px 0px 4px;
    font-size: 0.9em;
}

code {
    background-color: #f3f4f4;
    padding: 0 2px 0 2px;
}

.md-fences {
    margin-bottom: 15px;
    margin-top: 15px;
    padding-top: 8px;
    padding-bottom: 6px;
}


.md-task-list-item > input {
  margin-left: -1.3em;
}

@media print {
    html {
        font-size: 13px;
    }
    pre {
        page-break-inside: avoid;
        word-wrap: break-word;
    }
}

.md-fences {
	background-color: #f8f8f8;
}
#write pre.md-meta-block {
	padding: 1rem;
    font-size: 85%;
    line-height: 1.45;
    background-color: #f7f7f7;
    border: 0;
    border-radius: 3px;
    color: #777777;
    margin-top: 0 !important;
}

.mathjax-block>.code-tooltip {
	bottom: .375rem;
}

.md-mathjax-midline {
    background: #fafafa;
}

#write>h3.md-focus:before{
	left: -1.5625rem;
	top: .375rem;
}
#write>h4.md-focus:before{
	left: -1.5625rem;
	top: .285714286rem;
}
#write>h5.md-focus:before{
	left: -1.5625rem;
	top: .285714286rem;
}
#write>h6.md-focus:before{
	left: -1.5625rem;
	top: .285714286rem;
}
.md-image>.md-meta {
    /*border: 1px solid #ddd;*/
    border-radius: 3px;
    padding: 2px 0px 0px 4px;
    font-size: 0.9em;
    color: inherit;
}

.md-tag {
    color: #a7a7a7;
    opacity: 1;
}

.md-toc { 
    margin-top:20px;
    padding-bottom:20px;
}

.sidebar-tabs {
    border-bottom: none;
}

#typora-quick-open {
    border: 1px solid #ddd;
    background-color: #f8f8f8;
}

#typora-quick-open-item {
    background-color: #FAFAFA;
    border-color: #FEFEFE #e5e5e5 #e5e5e5 #eee;
    border-style: solid;
    border-width: 1px;
}

/** focus mode */
.on-focus-mode blockquote {
    border-left-color: rgba(85, 85, 85, 0.12);
}

header, .context-menu, .megamenu-content, footer{
    font-family: "Segoe UI", "Arial", sans-serif;
}

.file-node-content:hover .file-node-icon,
.file-node-content:hover .file-node-open-state{
    visibility: visible;
}

.mac-seamless-mode #typora-sidebar {
    background-color: #fafafa;
    background-color: var(--side-bar-bg-color);
}

.md-lang {
    color: #b4654d;
}

/*.html-for-mac {
    --item-hover-bg-color: #E6F0FE;
}*/

#md-notification .btn {
    border: 0;
}

.dropdown-menu .divider {
    border-color: #e5e5e5;
    opacity: 0.4;
}

.ty-preferences .window-content {
    background-color: #fafafa;
}

.ty-preferences .nav-group-item.active {
    color: white;
    background: #999;
}

.menu-item-container a.menu-style-btn {
    background-color: #f5f8fa;
    background-image: linear-gradient( 180deg , hsla(0, 0%, 100%, 0.8), hsla(0, 0%, 100%, 0)); 
}
.bluecolor {
    color: blue;
}


</style><title>LiYuanman CV</title>
</head>
<body class='typora-export os-windows'><div class='typora-export-content'>
<div id='write'  class=''><p><a name="funding"></a><a name="publications"></a><strong><h1>Dr. Yuanman Li</h1></strong></p><p><span>Email: </span><a href='mailto:yuanmanli@szu.edu.cn' target='_blank' class='url'>yuanmanli@szu.edu.cn</a><span>, </span><a href='mailto:yuanmanx.li@gmail.com' target='_blank' class='url'>yuanmanx.li@gmail.com</a><span>; Phone: (+86) 13927415980</span></p><p><span>Associate Research Fellow, College of Electronics and Information Engineering</span></p><p><span>Canghai Campus, Building N605, Shenzhen University, Shenzhen, China</span></p><p><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span><span>_</span></p><p><strong><span class="bluecolor">SUMMARY:</span></strong></p><p><img src="Aspose.Words.91b7a933-d1ef-4a01-8624-2926c51e275f.001.jpeg" referrerpolicy="no-referrer"><a name="ole_link14"></a><span>Dr. Yuanman Li is an Associate Research Fellow at the School of Electronics and Information Engineering and a member of IEEE. He earned his Bachelor&#39;s degree in Software Engineering from Chongqing University in 2012. After graduating, he was awarded a full scholarship to the University of Macau, where he obtained his Master&#39;s degree in Software Engineering in 2015 and a Ph.D. in Computer Science in 2018.</span></p><p><span>From September 2018 to August 2019, he worked as a postdoctoral researcher at the National Key Laboratory of Internet of Things for Smart City in Macau. Since September 2019, he has been serving at the School of Electronics and Information Engineering at Shenzhen University. His research focuses on multimedia information security and computer vision. He has been honored with titles such as “Shenzhen Overseas High-Level Talent” and “Shenzhen Outstanding Scientific and Technological Innovation Talent”.</span></p><p><span>Dr. Li has led more than ten projects, including National Natural Science Foundation projects, provincial natural science foundation projects, and corporate-funded projects. He has been granted six Chinese invention patents, and has published over 50 research papers in prestigious international journals and conferences such as T-IFS, T-SC, T-CSVT, T-MM, T-NNLS, T-NSE, SPL, CVPR, ACMMM, and AAAI. His research achievements were recognized with the 2022 Macau Natural Science Award.</span></p><p><strong><span class="bluecolor">EDUCATION:</span></strong></p><ul><li><p><span>PhD in Computer and Information Science, University of Macau, Macau (2015-2018)</span></p></li><li><p><span>Master in Software Engineering, University of Macau, Macau (2012-2015)</span></p></li><li><p><span>Bachelor in Software Engineering, Chongqing University, China (2008-2012)</span></p></li></ul><p><strong><span class="bluecolor">WORKING EXPERIENCE:</span></strong></p><ul><li><p><span>Associate Research Fellow, College of Electronics and Information Engineering, Shenzhen University, China (2021.06-)</span></p></li><li><p><span>Assistant Professor, College of Electronics and Information Engineering, Shenzhen University, China (2019.09-)</span></p></li><li><p><span>Post-Doc Fellow, the State Key Laboratory of Internet of Things for Smart City in Macau SAR, Macau (2018.09-2019.08)</span></p></li></ul><p><strong><span class="bluecolor">RESEARCH INTERESTS:</span></strong></p><ul><li><p><span>Multimedia Forensics and Security</span></p></li><li><p><span>Multimedia Signal Processing</span></p></li><li><p><span>Security and Privacy in Machine Learning</span></p></li><li><p><span>Computer Vision and Big Data</span></p></li></ul><p><strong><span class="bluecolor">COURSE TEACHING:</span></strong></p><ul><li><p><span>Machine Learning</span></p></li><li><p><span>Big Data Analysis</span></p></li><li><p><span>Specialty Comprehensive Design</span></p></li><li><p><span>Image and Audio-Video Processing Techniques and Cutting-Edge Applications</span></p></li></ul><p><strong><span class="bluecolor">AWARDS AND HONOR：</span></strong></p><ul><li><p><span>Macau Science and Technology Awards (3rd Prize, Natural Science Award), 2022</span></p></li><li><p><a name="ole_link15"></a><span>Shenzhen Outstanding Scientific and Technological Innovation Talent Award, 2022</span></p></li><li><p><span>Shenzhen University Research Excellence Award, 2022</span></p></li><li><p><span>CCF-Alibaba Innovation Research Program Young Talent, 2022</span></p></li><li><p><span>Shenzhen High-Level Overseas Talent Award, 2020</span></p></li><li><p><span>Macau Postgraduate Technology Research and Development Award, 2018</span></p></li><li><p><span>Outstanding graduate of Chongqing University, 2012</span></p></li><li><p><span>National Scholarship of the Ministry of Education of the People&#39;s Republic of China, 2011</span></p></li><li><p><span>The Second Prize of the National College Students Mathematical Modeling Contest, 2010</span></p></li></ul><p><strong><span class="bluecolor">PROFESSIONAL ACTIVITIES:</span></strong></p><ul><li><p><span>Member of IEEE, 2018-</span></p></li><li><p><span>Guest Editor, Visual Intelligence, 2023-</span></p></li><li><p><span>Chinese Society of Image and Graphics (CSIG) - Digital Forensics and Security Professional Committee Member, 2020-</span></p></li><li><p><span>Area Chair, IEEE International Conference on Multimedia and Expo (ICME), 2023</span></p></li><li><p><span>Session Chair, IEEE International Conference on Multimedia and Expo (ICME), 2023</span></p></li><li><p><span>Session Chair, International Conference Security and Privacy in Digital Economy (SPDE), 2023</span></p></li><li><p><span>Program Committee Member, AAAI, CVPR, ICCV and ACMMM, 2020-</span></p></li></ul><p><strong><span class="bluecolor">INVITED TALKS:</span></strong></p><ul><li><p><span>2023 3rd Western Region Comprehensive Governance and Academic Development Seminar on Cyberspace Security: Research on Robust Source Forensics for Short Videos from Multiple Social Networking Platforms</span></p></li><li><p><span>2022 Alibaba Business Security Department Workshop: Recent Research on Multimedia Security.</span></p></li><li><p><span>2021 1st Macau Smart City Technology Seminar: Operation Chain Detection for Digital Images.</span></p></li></ul><p><strong><span class="bluecolor">SELECTED PUBLICATIONS:</span></strong></p><p><strong><span class="bluecolor">JOURNAL</span></strong><span>:</span></p><ol start='' ><li><p><strong><span>Yuanman Li</span></strong><span>, Liangpei Hu, Li Dong, Haiwei Wu, Jinyu Tian, Jiantao Zhou and Xia Li, “Transformer-Based Image Inpainting Detection via Label Decoupling and Constrained Adversarial Training”, </span><em><strong><span>IEEE Transactions on Circuits and Systems for Video Technology (T-CSVT)</span></strong></em><span>, in press, 2023.</span></p></li><li><p><strong><span>Yuanman Li</span></strong><span>, Minhua Liu, Jinyu Tian, Jie Du, and Xia Li, “Operation History Estimation and Its Application to Multi-Degraded Image Restoration”, </span><em><strong><span>IEEE Transactions on Consumer Electronics (T-CE)</span></strong></em><span>, in press, 2023.</span></p></li><li><p><strong><span>Yuanman Li</span></strong><span>, Jiaxiang You, Jiantao Zhou, Wei Wang, Xin Liao and Xia Li, “Image Operation Chain Detection with Machine Translation Framework”, </span><em><strong><span>IEEE Transactions on Multimedia (T-MM)</span></strong></em><span>, in press, 2022.</span></p></li><li><p><strong><span>Yuanman Li</span></strong><span>, J. T. Zhou, J. Y. Tian, X. W. Zheng and Y. Y .Tang, “Weighted Error Entropy based Information Theoretic Learning for Robust Subspace Representation”, </span><em><strong><span>IEEE Transactions on Neural Networks and Learning Systems (T-NNLS)</span></strong></em><span>, vol. 33, no. 9, pp. </span><a name="ole_link2"></a><span>4228-4242, 2022.</span></p></li><li><p><strong><span>Yuanman Li</span></strong><span> and J. T. Zhou, “Fast and Effective Image Copy-Move Forgery Detection via Hierarchical Feature Point Matching”, </span><em><strong><span>IEEE Transactions on Information Forensics and Security (T-IFS)</span></strong></em><span>, vol. 14, no. 5, pp. 1307-1322, 2019.</span></p></li><li><p><strong><span>Yuanman Li</span></strong><span>, J. T. Zhou, and A. Cheng, “SIFT Keypoint Removal via Directed Graph Construction for Color Images”, </span><em><strong><span>IEEE Transactions on Information Forensics and Security (T-IFS)</span></strong></em><span>, vol. 12, no. 12, pp. 2971-2985, 2017.</span></p></li><li><p><strong><span>Yuanman Li</span></strong><span>, J. T. Zhou, A. Cheng, X. M. Liu, and Y. Y. Tang, “SIFT Keypoint Removal and Injection via Convex Relaxation”, </span><em><strong><span>IEEE Transactions on Information Forensics and Security (T-IFS)</span></strong></em><span>, vol. 11, no. 8, pp. 1722-1735, 2016.</span></p></li><li><p><strong><span>Yuanman Li</span></strong><span>, Jiantao Zhou and Xia Li, “Robust Matrix Factorization via Minimum Weighted Error Entropy Criterion”, </span><em><strong><span>IEEE Transactions on Computational Social Systems (T-CSS)</span></strong></em><span>, vol.9, no.6, pp.1830-1841, 2022.</span></p></li><li><p><strong><span>Yuanman Li</span></strong><span>, R. Q. Liang, W. Wei, W. Wang, J. T. Zhou and X. Li, “Temporal Pyramid Network with Spatial-Temporal Attention for Pedestrian Trajectory Prediction”, </span><em><strong><span>IEEE Transactions on Network Science and Engineering (T-NSE)</span></strong></em><span>, vol.9, no.3, pp.1006-1019, 2022.</span></p></li><li><p><strong><span>Yuanman Li</span></strong><span>, Ce Xie, Rongqin Liang, Jie Du, Jiantao Zhou and Xia Li, “A Synchronous Bi-Directional Framework With Temporally Dependent Interaction Modeling for Pedestrian Trajectory Prediction”, </span><em><strong><span>IEEE Transactions on Network Science and Engineering (T-NSE)</span></strong></em><span>, in press, 2023.</span></p></li><li><p><strong><span>Yuanman Li</span></strong><span> and J. T. Zhou, “Anti-Forensics of Lossy Predictive Image Compression”, </span><em><strong><span>IEEE Signal Processing Letters (SPL)</span></strong></em><span>, vol. 22, no. 12, pp. 2219-2223, 2015.</span></p></li><li><p><span>Rongqin Liang, </span><strong><span>Yuanman Li</span></strong><span>*</span><span>, Ce Xie, Rongqin Liang, Jie Du, Jiantao Zhou and Xia Li, “STGlow: A Flow-Based Generative Framework With Dual-Graphormer For Pedestrian Trajectory Prediction”, </span><em><strong><span>IEEE Transactions on Neural Networks and Learning Systems (T-NNLS)</span></strong></em><span>, in press, 2023.</span></p></li><li><p><span>Jie Du, Kai Guan, Yanhong Zhou, </span><strong><span>Yuanman Li</span></strong><span>*</span><span> and Tianfu Wang, “Parameter-Free Similarity-Aware Attention Module for Medical Image Classification and Segmentation”, </span><em><strong><span>IEEE Transactions on Emerging Topics in Computational Intelligence (T-ETCI)</span></strong></em><span>, pp.845-857, 2023.</span></p></li><li><p><span>Zhongyun Hua, Ziyi Wang, Yifeng Zheng, Yongyong Chen and </span><strong><span>Yuanman Li</span></strong><span>*</span><span>, </span><a name="_hlk148717915"></a><span>“</span><a href='https://scholar.google.com/citations?view_op=view_citation&amp;hl=zh-CN&amp;user=GQUDu68AAAAJ&amp;sortby=pubdate&amp;citation_for_view=GQUDu68AAAAJ:dhFuZR0502QC'><span>Enabling Large-Capacity Reversible Data Hiding Over Encrypted JPEG Bitstreams</span></a><span>”, </span><em><strong><span>IEEE Transactions on Circuits and Systems for Video Technology (T-CSVT)</span></strong></em><span>, vol. 33, no. 3,pp.1003-1018, 2023.</span></p></li><li><p><span>Jie Du, Kai Guan, Peng Liu,</span><strong><span>Yuanman Li</span></strong><span>*</span><span> and Tianfu Wang, “Boundary-Sensitive Loss Function With Location Constraint for Hard Region Segmentation”, </span><em><strong><span>IEEE Journal of Biomedical and Health Informatics (JBHI)</span></strong></em><span>, pp.992-1003, 2023.</span></p></li><li><p><span>Junyang Chen, Zhiguo Gong,</span><strong><span>Yuanman Li</span></strong><span>*</span><span>, Huanjian Zhang, Hongyong Yu, JunzhangZhu, Ge Fan, Xiao-Ming Wu and Kaishun Wu, “Meta-Path Based Neighbors for Behavioral Target Generalization in Sequential Recommendation”, </span><em><strong><span>IEEE Transactions on Network Science and Engineering (T-NSE)</span></strong></em><span>, pp.1658-1667, 2022.</span></p></li><li><p><span>Kuiyuan Zhang, Zhongyun Hua, </span><strong><span>Yuanman Li</span></strong><span>, Yongyong Chen and Yicong Zhou, “AMS-Net: Adaptive Multi-Scale Network for Image Compressive Sensing”, </span><em><strong><span>IEEE Transactions on Multimedia (T-MM)</span></strong></em><span>, in press, 2022.</span></p></li><li><p><span>Junyang Chen, Xueliang Li, </span><strong><span>Yuanman Li</span></strong><span>, Paul Li, Mengzhu Wang, Xiang Zhang, Zhiguo Gong, Kaishun Wu and Victor C.M. Leung, “A Simple Yet Effective Layered Loss for Pre-Training of Network Embedding”, </span><em><strong><span>IEEE Transactions on Network Science and Engineering (T-NSE)</span></strong></em><span>, pp.1827-1837, 2022.</span></p></li><li><p><span>Y. C. Su, J. Du, </span><strong><span>Yuanman Li</span></strong><span>*</span><span>, X. Li, Z. Y. Hua and J. T Zhou, “Trajectory Forecasting Based on Prior-Aware Directed Graph Convolutional Neural Network”, </span><em><strong><span>IEEE Transactions on Intelligent Transportation Systems (T-ITS)</span></strong></em><span>, vol.23, no.9, pp. 16773-16785, 2022.</span></p></li><li><p><span>K. Y. Zhang, Z. Y. Hua, </span><strong><span>Yuanman Li</span></strong><span>, Y. S. Zhang and Y. C. Zhou, &quot;Uformer-ICS: A U-Shaped Transformer for Image Compressive Sensing Service,&quot; </span><em><strong><span>IEEE Transactions on Services Computing</span></strong></em><span>, accepted, 2023. </span></p></li><li><p><span>H. W. Wu, J. T. Zhou and </span><strong><span>Yuanman Li</span></strong><span>, “Deep Generative Model for Image Inpainting with Local Binary Pattern Learning and Spatial Attention”, </span><em><strong><span>IEEE Transactions on Multimedia (T-MM)</span></strong></em><span>, vol. 24, pp. 4016-4027, 2022.</span></p></li><li><p><span>W. W. Sun, J. T. Zhou, </span><strong><span>Yuanman Li</span></strong><span>, M. Cheung and J. She, “Robust High Capacity Watermarking over Online Social Network Shared Images”, </span><em><strong><span>IEEE Transactions on Circuits and Systems for Video Technology (T-SCVT)</span></strong></em><span>, vol.31, pp.1208-1221, 2021.</span></p></li><li><p><span>J. Duan, J. T. Zhou and </span><strong><span>Yuanman Li</span></strong><span>, “Secure and Verifiable Outsourcing of Large-scale Nonnegative Matrix Factorization (NMF)”, </span><em><strong><span>IEEE Transactions on Services Computing (T-SC)</span></strong></em><span>, vol. 14, no. 6, pp. 1940-1953,2021.</span></p></li><li><p><span>Z. Y. Hua, K. Y. Zhang, </span><strong><span>Yuanman Li</span></strong><span> and Y. C. Zhou, “Visually secure image encryption using adaptive-thresholding sparsification and parallel compressive sensing”, </span><em><strong><span>Signal Processing</span></strong></em><span>, vol.183, pp. 107998, 2021.</span></p></li><li><p><span>Z. Y. Hua, Z. H. Zhu, Y. Y. Chen and </span><strong><span>Yuanman Li</span></strong><span> , “Color image encryption using orthogonal Latin squares and a new 2D chaotic system”, </span><em><strong><span>Nonlinear Dynamics</span></strong></em><span>, vol.104, pp. 4505-4522, 2021.</span></p></li><li><p><span>Z. Y. Hua, J. X. Li, </span><strong><span>Yuanman Li</span></strong><span> and Y. Y. Chen “Image encryption using value-differencing transformation and modified ZigZag transformation”, </span><em><strong><span>Nonlinear Dynamics</span></strong></em><span>, vol.106, pp.3583-3599, 2021.</span></p></li><li><p><span>J. Duan, J. T. Zhou and </span><strong><span>Yuanman Li</span></strong><span>, “Privacy-Preserving distributed deep learning based on secret sharing”, </span><em><strong><span>Information Science</span></strong></em><span>, vol. 527, pp. 108-127, 2020.</span></p></li></ol><p><strong><span class="bluecolor">CONFERENCE</span></strong></p><ol start='' ><li><p><strong><span>Yuanman Li</span></strong><span>, J. T. Zhou, X. W. Zheng, J. Y. Tian and Y. Y. Tang, “Robust Subspace Clustering with Independent and Piecewise Identically Distributed (i.p.i.d.) Noise Modeling”, </span><em><strong><span>IEEE Conf. Comput. Vis. and Pattern Recogn. (CVPR)</span></strong></em><span>, 2019 (oral, top 5% paper).</span></p></li><li><p><span>Yuxuan Tan, </span><strong><span>Yuanman Li</span></strong><span>*</span><span>, Limin Zeng, Jiaxiong Ye, Wei Wang and Xia Li, “Multi-scale Target-Aware Framework for Constrained Image Splicing Detection and Localization”, </span><em><strong><span>ACM Multimedia (ACM MM)</span></strong></em><span>, 2023.</span></p></li><li><p><span>J. X. You, </span><strong><span>Yuanman Li</span></strong><span>*</span><span>, J. T. Zhou, Z. Y. Hua, W. W. Sun and X. Li, “A Transformer based Approach for Image Manipulation Chain Detection”, </span><em><strong><span>ACM Multimedia (ACM MM)</span></strong></em><span>, 2021.</span></p></li><li><p><span>R. Q. Liang, </span><strong><span>Yuanman Li</span></strong><span> </span><span>*</span><span>, X. Li, Y. Tang, J. T. Zhou and W. B. Zou, “Temporal Pyramid Network for Pedestrian Trajectory Prediction with Multi-Supervision”, </span><em><strong><span>AAAI Conference on Artificial Intelligence (AAAI)</span></strong></em><span>, 2021.</span></p></li><li><p><span>J. Y. Tian, J. T. Zhou, </span><strong><span>Yuanman Li</span></strong><span> and J. Duan, “Detecting Adversarial Examples from Sensitivity Inconsistency of Spatial-Transform Domain”, </span><em><strong><span>AAAI Conference on Artificial Intelligence (AAAI)</span></strong></em><span>, 2021.</span></p></li><li><p><span>Y. Y. Li, J. T. Zhou, and </span><strong><span>Yuanman Li</span></strong><span>, “Ciphertext-Only Attack on an Image Homomorphic Encryption Scheme with Small Ciphertext Expansion”, </span><em><span>In Proceedings of ACM Multimedia (</span><strong><span>ACM MM</span></strong><span>)</span></em><span>, 2015.</span></p></li><li><p><span>Yingjie He, </span><strong><span>Yuanman Li</span></strong><span>*</span><span>, Changsheng Chen and Xia Li, “Image Copy-Move Forgery Detection via Deep CrossScale PatchMatch”, </span><em><strong><span>IEEE International Conference on Multimedia and Expo (ICME)</span></strong></em><span>, pp.2327-2332, 2023 (oral, top 15% paper).</span></p></li><li><p><span>Minhua Liu, </span><strong><span>Yuanman Li</span></strong><span>*</span><span>, Rongqin Liang, Jiaxiang You and Xia Li, “Multiple Degraded Image Restoration via Degradation History Estimation”, </span><em><strong><span>IEEE International Conference on Multimedia and Expo (ICME)</span></strong></em><span>, pp.528-533, 2023 (oral, top 15% paper).</span></p></li><li><p><span>Jiaxiang You, </span><strong><span>Yuanman Li</span></strong><span>*</span><span>, Rongqin Liang, Yuxuan Tan, Jiantao Zhou and Xia Li, “Image Sharing Chain Detection via Sequence-To-Sequence Model”, </span><em><strong><span>IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span></strong></em><span>, pp.1-6, 2023 (oral, top 15% paper).</span></p></li><li><p><span>Ce Xie, </span><strong><span>Yuanman Li</span></strong><span>*</span><span>, Rongqin Liang, Li Dong and Xia Li, “Synchronous Bi-directional Pedestrian Trajectory Prediction with Error Compensation”, </span><em><strong><span>Asian Conference on Computer Vision (ACCV)</span></strong></em><span>, pp.2796-2812, 2022.</span></p></li><li><p><span>Liangpei Hu, </span><strong><span>Yuanman Li</span></strong><span>*</span><span>,Jiaxiang You, Rongqin Liang and Xia Li, “An Edge-Aware Transformer Framework for Image Inpainting Detection”, </span><em><strong><span>International Conference on Artificial Intelligence and Security (ICAIS)</span></strong></em><span>, pp.648-660, 2022.</span></p></li><li><p><span>Weipeng Liang, Li Dong, Rangding Wang, Diqun Yan and </span><strong><span>Yuanman Li</span></strong><span>, “Robust Document Image Forgery Localization Against Image Blending”, </span><em><strong><span>IEEE International Conference on Trust, Security and Privacy in Computing and Communications (TrustCom)</span></strong></em><span>, pp.810-817, 2022.</span></p></li><li><p><span>Li Dong, Jie Wang, Rangding Wang, </span><strong><span>Yuanman Li</span></strong><span> and Weiwei Sun “Towards Image Data Hiding via Facial Stego Synthesis with Generative Model”, </span><em><strong><span>International Joint Conference on Artificial Intelligence - International Workshop on Safety &amp; Security of Deep Learning (IJCAI -Workshop)</span></strong></em><span>, in press, 2021.</span></p></li><li><p><span>W. Wang, B. X. Lu, </span><strong><span>Yuanman Li</span></strong><span>, W. Wei, J. Q. Li, S. Mumtaz and M. Guizani, “Task Scheduling Game Optimization for Mobile Edge Computing”, </span><em><strong><span>IEEE International Conference on Communications (ICC)</span></strong></em><span>, in press, 2021.</span></p></li><li><p><span>H.W.WU, J.T. Zhou, </span><strong><span>Yuanman Li</span></strong><span>, “Image Reconstruction from Local Descriptors Using Conditional Adversarial Networks“, </span><em><span>APSIPA Annual Summit and Conference (ASC)</span></em><span>, 2019. (</span><strong><span>Oral</span></strong><span>)</span></p></li><li><p><strong><span>Yuanman Li</span></strong><span> and J. T. Zhou, “Image Copy-Move Forgery Detection Using Hierarchical Feature Point Matching”, </span><em><span>APSIPA Annual Summit and Conference (ASC)</span></em><span>, 2016. (</span><strong><span>Oral</span></strong><span>)</span></p></li><li><p><span>J. Duan, J. T. Zhou, and </span><strong><span>Yuanman Li</span></strong><span>, “Secure and Verifiable Outsourcing of Nonnegative Matrix Factorization (NMF)”, </span><em><strong><span>ACM Workshop on Information Hiding and Multimedia Security (IH&amp;MMSec-16)</span></strong></em><span>, 2016.</span></p></li><li><p><span>A. Cheng, </span><strong><span>Yuanman Li</span></strong><span>, and J. T. Zhou, “SIFT Keypoint Removal via Convex Relaxation”, </span><em><strong><span>IEEE International Conference on Multimedia and Expo (ICME-15)</span></strong></em><span>, 2015 (oral, top 15% paper).</span></p></li><li><p><span>Y. Y. Li, J. T. Zhou, </span><strong><span>Yuanman Li</span></strong><span>, and O. C. Au, “Reducing the Ciphertext Expansion in Image Homomorphic Encryption via Linear Interpolation Technique”, </span><em><span>IEEE Global Conference on Signal and Information Processing (GlobalSIP)</span></em><span>, 2015.</span></p></li><li><p><strong><span>Yuanman Li</span></strong><span> and J. T. Zhou, “Sparsity-driven reconstruction of L</span><span>_</span><span>\infinity-decoded images”,</span><em><strong><span>IEEE International Conference on Image Processing (ICIP)</span></strong></em><span>, 2014.</span></p></li><li><p><span>L. Dong, J. Wang, </span><strong><span>Yuanman Li</span></strong><span> and Y. Y. Tang,“Sector projection fourier descriptor for Chinese character recognition”, </span><em><span>IEEE International Conference on Cybernetics (CYBCONF)</span></em><span>, 2013.</span></p></li></ol></div></div>
</body>
</html>